---
title: "Decision log for next word prediction"
author: "Chris Lill"
date: "12 December 2015"
output: html_document
---

# Task 0 - Understanding the problem

## Tasks to accomplish 
1. Obtaining the data - Can you download the data and load/manipulate it in R?

2. Familiarizing yourself with NLP and text mining - Learn about the basics of natural language processing and how it relates to the data science process you have learned in the Data Science Specialization. 

## Questions to consider 
1. What do the data look like?
* Three character vectors where each element is a paragraph.

2. Where do the data come from?
* It's scraped from the web, from blogs, news and tweets.

3. Can you think of any other data sources that might help you in this project?
* List of profanities
* Common encoding errors, such as in blogs[34556]
* Rules for stemming (if appropriate)

4. What are the common steps in natural language processing?
* Organize and import data
* Tidy data
  + text reformatting
  + stopword removal
  + stemming procedures
* Transform data - create a term-document matrix

5. What are some common issues in the analysis of text data?
* Data cleanliness
* Complexity of human language
* Context (should we cache?)
* Memory (we can't hold everything in memory)

6. What is the relationship between NLP and the concepts you have learned in the Specialization? 
* NLP has a set of activities to clean and structure data. Once this is done we can apply machine learning techniques.

## Research
* https://class.coursera.org/nlp/lecture/preview
* http://www.jstatsoft.org/index.php/jss/article/view/v025i05/v25i05.pdf
* Weka gives us stemming and tokenization methods
  + OpenNLP offers amongst others tokenization, sentence detection, and part of speech tagging
  + term-document matrices uses a bag-of-words mechanism which means that the order of tokens is irrelevant
  + could use an n-gram tokenizer 
    + TermDocMatrix(col, control = list(tokenize = NGramTokenizer))
    + TermDocMatrix(col, control = list(tokenize = tokenize))
    + TermDocMatrix(col, control = list(tokenize = sentDetect))
  + we can use external modules for all other processing steps mainly using termFreq 
    + stemming 
    + stopword removal (e.g., via custom stopword lists)
    + user supplied dictionaries (a method to restrict the generated terms in the term-document matrix)
  + Weightings can be used
  + Creating a Corpus gives us better control of text
    + Can use dbControl to solve memory problems
  + Transformations
    + tmMap(ovid, FUN = tmTolower)
    + removeNumbers() 
    + removePunctuation()
    + removeWords() 
    + replaceWords()
    + stemDoc()
    + stripWhitespace()
  + Filters (tmIndex() or tmFilter())
    + tmFilter(ovid, FUN = searchFullText, "Venus", doclevel = TRUE)
    + Default sFilter()
  + stopwords(language = en)
  + Part of speech tagging
    + library (openNLP)
    + tagPOS()
  + Useful functions
    + findFreqTerms()
    
    
# Task 1 - Data acquisition and cleaning

## Tasks to accomplish 
1. Tokenization - identifying appropriate tokens such as words, punctuation, and numbers. Writing a function that takes a file as input and returns a tokenized version of it. 
2. Profanity filtering - removing profanity and other words you do not want to predict. 
3. Quiz 1

## Questions to consider 
1. How should you handle punctuation?
2. The data contains lots of times, dates, numbers and currency values. How to handle these? Are they useful for prediction?
3. How do you find typos in the data?
4. How do you identify garbage, or the wrong language?
5. How do you define profanity? How do you ensure you don't remove words you want to include?
6. How do you handle capital and lower cases? 
7. What is the best set of features you might use to predict the next word? 




